{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05495d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "710eabad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahuloruganti/Documents/git/RAG-based-mini-system-to-automate-test-cases/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chunks (lan_chunks): 150\n"
     ]
    }
   ],
   "source": [
    "# Recreate the same chunking logic from chunking.ipynb\n",
    "py_file = Path(\"./docs/gmail_pytest_suite.py\")\n",
    "source = py_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Use the same language-aware splitter configuration\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Create lan_chunks (same as chunking notebook)\n",
    "lan_chunks = splitter.split_text(source)\n",
    "\n",
    "# Import the same embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"   chunks (lan_chunks): {len(lan_chunks)}\")\n",
    "\n",
    "# Store for use in vector databases\n",
    "documents = lan_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d047f3",
   "metadata": {},
   "source": [
    "# ChromaDB Implementation\n",
    "\n",
    "ChromaDB provides an easy-to-use vector database that integrates seamlessly with LangChain. Perfect for prototyping and local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a75a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Collection: language_chunks\n",
      "‚úÖ Successfully stored 150 language-aware chunks in ChromaDB!\n",
      "‚úÖ Successfully stored 150 language-aware chunks in ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(\"./data/chromadb\", exist_ok=True)\n",
    "\n",
    "# Initialize ChromaDB client with persistent storage\n",
    "chroma_client = chromadb.PersistentClient(path=\"./data/chromadb\")\n",
    "\n",
    "# Create or get collection for language-aware chunks\n",
    "collection_name = \"language_chunks\"\n",
    "try:\n",
    "    # Delete existing collection if it exists (for clean restart)\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "# Create LangChain wrapper for seamless integration\n",
    "vectorstore = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "print(f\"   Collection: {collection_name}\")\n",
    "\n",
    "# Create metadata for each chunk\n",
    "metadatas = []\n",
    "for i, doc in enumerate(documents):\n",
    "    metadatas.append({\n",
    "        \"chunk_id\": i,\n",
    "        \"chunk_size\": len(doc),\n",
    "        \"chunking_method\": \"language_aware\",\n",
    "        \"source\": \"gmail_pytest_suite.py\",\n",
    "        \"chunk_type\": \"python_code\"\n",
    "    })\n",
    "\n",
    "# Add documents to the vector store\n",
    "vectorstore.add_texts(\n",
    "    texts=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=[f\"lang_chunk_{i}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Successfully stored {len(documents)} language-aware chunks in ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a168df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing ChromaDB similarity search...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç Testing ChromaDB similarity search...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mtest login with valid credentials\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43mvectorstore\u001b[49m.similarity_search(query, k=\u001b[32m3\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m similar documents:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test similarity search\n",
    "print(\"\\nüîç Testing ChromaDB similarity search...\")\n",
    "query = \"test login with valid credentials\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Found {len(results)} similar documents:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- ChromaDB Result {i} ---\")\n",
    "    print(f\"Content preview: {result.page_content[:150]}...\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n",
    "\n",
    "# Test similarity search with scores\n",
    "print(\"\\nüìä ChromaDB similarity search with scores...\")\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Results with similarity scores:\")\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n--- Result {i} (Score: {score:.4f}) ---\")\n",
    "    print(f\"Preview: {doc.page_content[:500]}...\")\n",
    "    print(f\"Chunk size: {doc.metadata['chunk_size']} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136fe73",
   "metadata": {},
   "source": [
    "# FAISS Implementation\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) provides high-performance vector similarity search. Excellent for production use cases requiring fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cae818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generated 150 embeddings\n",
      "   Embedding dimension: 384\n",
      "   Total vectors: 150\n",
      "   Index dimension: 384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(\"./data/faiss\", exist_ok=True)\n",
    "\n",
    "# Generate embeddings for all language-aware chunks\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "embeddings_array = np.array(document_embeddings, dtype='float32')\n",
    "\n",
    "# Create FAISS index (using L2 distance for exact search)\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings_array)\n",
    "\n",
    "print(f\"   Total vectors: {index.ntotal}\")\n",
    "print(f\"   Index dimension: {dimension}\")\n",
    "\n",
    "# Save FAISS index and document mapping\n",
    "faiss_data_dir = \"./data/faiss\"\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss.write_index(index, f\"{faiss_data_dir}/language_chunks_index.faiss\")\n",
    "\n",
    "# Save document mapping and metadata\n",
    "faiss_data = {\n",
    "    'documents': documents,\n",
    "    'metadatas': metadatas,\n",
    "    'embeddings_array': embeddings_array,\n",
    "    'chunking_method': 'language_aware'\n",
    "}\n",
    "\n",
    "with open(f\"{faiss_data_dir}/language_chunks_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(faiss_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c74707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS similarity search (inline, no functions or print statements)\n",
    "query = \"test login with valid credentials\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "query_vector = np.asarray([query_embedding], dtype='float32')\n",
    "\n",
    "distances, indices = index.search(query_vector, 3)\n",
    "\n",
    "faiss_results = []\n",
    "for dist, idx in zip(distances[0], indices[0]):\n",
    "    if idx < 0:\n",
    "        continue\n",
    "    doc = documents[idx]\n",
    "    meta = metadatas[idx]\n",
    "    faiss_results.append({\n",
    "        \"index\": int(idx),\n",
    "        \"distance\": float(dist),\n",
    "        \"score\": 1.0 / (1.0 + float(dist)),\n",
    "        \"preview\": doc[:200],\n",
    "        \"chunk_size\": meta.get(\"chunk_size\")\n",
    "    })\n",
    "\n",
    "faiss_results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
