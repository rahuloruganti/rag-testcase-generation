{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010421a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§© Chunks created: 150\n",
      "def test_login_unregistered_email_002(requests_mock):\n",
      "    \"\"\"unregistered email rejected\"\"\"\n",
      "    url = \"https://accounts.google.com/v3/signin/verify\"\n",
      "    payload = {\"email\": \"nouser@example.com\", \"password\": \"any_pw\"}\n",
      "    requests_mock.post(url, status_code=404)\n",
      "    import requests\n",
      "    resp = requests.post(url, json=payload)\n",
      "    assert resp.status_code == 404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# Use the Gmail pytest suite from docs explicitly\n",
    "py_file = Path(\"./docs/gmail_pytest_suite.py\")\n",
    "source = py_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Configure a language-aware splitter for Python code\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Split into overlapping chunks (character-based, language-aware)\n",
    "lan_chunks = splitter.split_text(source)\n",
    "\n",
    "print(f\"ðŸ§© Chunks created: {len(lan_chunks)}\")\n",
    "print(lan_chunks[1])\n",
    "\n",
    "language_documents=lan_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd03b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in file: 52648\n",
      "ðŸ§© Chunks created: 161\n",
      "# Gmail Login â€“ 150 PyTest Cases\n",
      "\n",
      "def test_login_valid_001(requests_mock):\n",
      "    \"\"\"valid login redirects to inbox\"\"\"\n",
      "    url = \"https://accounts.google.com/v3/signin/verify\"\n",
      "    payload = {\"email\": \"user@example.com\", \"password\": \"correct_pw\"}\n",
      "    requests_mock.post(url, status_code=200)\n",
      "    import requests\n",
      "    resp = requests.post(url, json=payload)\n",
      "    assert resp.status_code == 200\n",
      "\n",
      "\n",
      "def test_login\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# Use the Gmail pytest suite from docs explicitly\n",
    "py_file = Path(\"./docs/gmail_pytest_suite.py\")\n",
    "\n",
    "source = py_file.read_text(encoding=\"utf-8\")\n",
    "print(f\"Characters in file: {len(source)}\")\n",
    "\n",
    "# Initialize tiktoken encoder (used for tokenization)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "source_tokens = enc.encode(source)\n",
    "\n",
    "# Configure a token-aware splitter using tiktoken encoding\n",
    "splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",  # tiktoken encoding\n",
    "    chunk_size=100,                # in tokens\n",
    "    chunk_overlap=20               # in tokens\n",
    ")\n",
    "\n",
    "# Split into overlapping token chunks\n",
    "tik_chunks = splitter.split_text(source)\n",
    "chunk_token_counts = [len(enc.encode(c)) for c in tik_chunks]\n",
    "\n",
    "print(f\"ðŸ§© Chunks created: {len(tik_chunks)}\")\n",
    "print(tik_chunks[0])\n",
    "\n",
    "# Expose for downstream steps\n",
    "token_documents = tik_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f9eb7",
   "metadata": {},
   "source": [
    "## Embedding-based SemanticChunker (LangChain Experimental)\n",
    "\n",
    "Now we use the embedding-driven SemanticChunker from langchain_experimental with HuggingFace embeddings and compare results against earlier strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc8766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-huggingface langchain-experimental sentence-transformers transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0560df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahuloruganti/Documents/git/rag-testcase-generation/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic chunks: 1\n",
      "First chunk preview:\n",
      "# Gmail Login â€“ 150 PyTest Cases\n",
      "\n",
      "def test_login_valid_001(requests_mock):\n",
      "    \"\"\"valid login redirects to inbox\"\"\"\n",
      "    url = \"https://accounts.google.com/v3/signin/verify\"\n",
      "    payload = {\"email\": \"user@example.com\", \"password\": \"correct_pw\"}\n",
      "    requests_mock.post(url, status_code=200)\n",
      "    import requests\n",
      "    resp = requests.post(url, json=payload)\n",
      "    assert resp.status_code == 200\n",
      "\n",
      "\n",
      "def test_login_unregistered_email_002(requests_mock):\n",
      "    \"\"\"unregistered email rejected\"\"\"\n",
      "    url = \"https://accounts.google.com/v3/signin/verify\"\n",
      "    payload = {\"email\": \"nouser@example.com\", \"password\": \"any_pw\"}\n",
      "    requests_mock.post(url, status_code=404)\n",
      "    import requests\n",
      "    resp = requests.post(url, json=payload)\n",
      "    assert resp.status_code == 404\n",
      "\n",
      "\n",
      "def test_login_valid_003(requests_mock):\n",
      "    \"\"\"valid login redirects to inbox\"\"\"\n",
      "    url = \"https://accounts.google.com/v3/signin/verify\"\n",
      "    payload = {\"email\": \"user@example.com\", \"password\": \"correct_pw\"}\n",
      "    requests_mock.post(url, status_code\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the semantic chunker (tune params as needed)\n",
    "sem_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"absolute\",    \n",
    "    breakpoint_threshold_amount=0.3,         # sensitivity for breakpoints\n",
    ")\n",
    "\n",
    "# Produce semantic chunks from `source`\n",
    "sem_chunks = sem_chunker.split_text(source)\n",
    "\n",
    "\n",
    "print(f\"Semantic chunks: {len(sem_chunks)}\")\n",
    "print(f\"First chunk preview:\\n{sem_chunks[0][:1000]}\")\n",
    "\n",
    "\n",
    "# Expose for downstream steps\n",
    "semantic_documents = sem_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48637b10",
   "metadata": {},
   "source": [
    "### Why Semantic Chunking Produces Only One Chunk\n",
    "\n",
    "The pytest test cases have very similar semantic content - they all follow the same pattern (mock setup, API call, assertion). Since the embeddings are nearly identical for adjacent test functions, SemanticChunker doesn't detect meaningful breakpoints and treats the entire content as one semantically coherent chunk. For test code, syntactic chunking (by function boundaries) is more effective than semantic chunking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
